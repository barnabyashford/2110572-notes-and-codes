Page 1:
Token Classification
PoS and NER
HMM, CRF, and search

Page 2:
Token Classification
‚Ä¢
A broad term for classifying tokens: PoS, NER
This lecture
Tokenization
Word embeddings

Page 3:
Part-Of-Speech tagging
‚Ä¢ Categorize words into similar 
grammatical properties (syntax)
‚Ä¢ Examples: Nouns, Verbs, 
Adjectives
‚Ä¢ Actual applications often use 
more granular PoS labels
‚Ä¢ PoS tags are often
‚Ä¢ Language specific
‚Ä¢ Application/corpus specific
https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html

Page 4:
Part-Of-Speech tagging
‚Ä¢ Input
‚Ä¢ They refuse to permit us to obtain the refuse permit.
‚Ä¢ Output
‚Ä¢ They/PRP refuse/VBP to/To permit/VB us/PRP to/TO 
obtain/VB the/DT refuse/NN permit/NN 
Example from MIT 6.864

Page 5:
PoS usage
‚Ä¢ Word disambiguation
‚Ä¢ Different word vectors for different PoS of the same words
‚Ä¢ Helps other NLP tasks
‚Ä¢ PoS provides additional information that helps other tasks
‚Ä¢ Tokenization
‚Ä¢ Name-Entity Recognition
‚Ä¢ Identify group of words that refer to the same entity
‚Ä¢ ‡∏•‡∏∏‡∏á‡∏û‡∏•‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏¢
‚Ä¢ [‡∏•‡∏∏‡∏á‡∏û‡∏•]/person ‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á[‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏¢]/title
‚Ä¢ Parsing Ex parsing name and address from a sentence
‚Ä¢ Search Ex disambiguation in keywords
‚Ä¢ Text-to-speech Ex Nice, France vs nice french toast.

Page 6:
Thai PoS standards
‚Ä¢
https://pythainlp.org/dev-docs/api/tag.html
‚Ä¢
LST20 and Orchid are most famous Thai datasets
‚Ä¢
There is also Universal POS tags (not used much for 
Thai)

Page 7:
Orchid PoS corpus
‚Ä¢ Building A Thai Part-Of-Speech Tagged Corpus 
(ORCHID) (1999) 
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.
34.3496
‚Ä¢ 47 tags

Page 8:

Page 9:
NER
‚Ä¢ Name-Entity Recognition wants to extract all name 
entities in a sentence and classify into types
‚Ä¢ Can tag groups of words into the same category
‚Ä¢ ‡∏•‡∏∏‡∏á‡∏û‡∏•‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏¢
‚Ä¢ [‡∏•‡∏∏‡∏á‡∏û‡∏•]/person ‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á[‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏¢]/title
Biomedical publication mining
IL-2 gene expression and NF-kappa B 
activation through CD28 requires 
reactive oxygen production by            
5-lipoxygenase.

Page 10:
NER in applications
‚Ä¢ Classifying/tagging content
‚Ä¢ Information retrieval
‚Ä¢ Trends analysis
https://blog.paralleldots.com/product/applications-named-entity-recognition-api/

Page 11:
Labeling standard (IOB format)
‚Ä¢
Consider the following sentence and NE tags
‡∏•‡∏∏‡∏á‡∏û‡∏•‡∏õ‡πã‡∏≤‡πÄ‡∏ö‡∏¥‡∏£‡πå‡∏î‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏ä‡∏≤‡∏ï‡∏¥
‡πÑ‡∏ó‡∏¢
‚Ä¢
The two names are merged into a single entity. To 
separate the two entities, we sometimes add B 
(beginning) and I (inside) to the tags
‡∏•‡∏∏‡∏á‡∏û‡∏•‡∏õ‡πã‡∏≤‡πÄ‡∏ö‡∏¥‡∏£‡πå‡∏î‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏ä‡∏≤‡∏ï‡∏¥
‡πÑ‡∏ó‡∏¢
Name
Name
Name
Name
Other
Song
NameB
NameI
NameB
NameI
Other
SongB
Song
SongI
Song
SongI

Page 12:
Thai Nested NER
‚Ä¢
Thai NER has multiple levels/layers and can be nested
https://medium.com/airesearch-in-th/thai-n-ner-thai-nested-named-entity-recognition-1969f8fe91f0

Page 13:
Thai Nested NER
https://aclanthology.org/2022.findings-acl.116/
https://github.com/vistec-AI/Thai-NNER
1 model per layer
1 model all layer

Page 14:
Overview
‚Ä¢ What is Token Classification?
‚Ä¢ Traditional methods
‚Ä¢ Sequence methods
‚Ä¢ HMM
‚Ä¢ CRF
‚Ä¢ Viterbi and beam search
‚Ä¢ Neural network methods

Page 15:
Sequence methods
‚Ä¢ They refuse to permit us to obtain the refuse permit.
‚Ä¢ They/PRP refuse/VBP to/To permit/VB us/PRP to/TO 
obtain/VB the/DT refuse/NN permit/NN 
‚Ä¢ Determining the PoS tag depends on the decision of the 
words around it
‚Ä¢ A sequence problem

Page 16:
Problem setup
‚Ä¢ Sequence of words
‚Ä¢ W:= {w1,w2,w3,‚Ä¶,wn}
‚Ä¢ Sequence of tags
‚Ä¢ T:= {t1,t2,t3,‚Ä¶tn}
‚Ä¢ Given W predict T
‚Ä¢ argmaxT P(T|W)
‚Ä¢ Or
‚Ä¢ argmaxT P(T,W)      =   argmaxT P(T,W)
P(W)
Discriminative Model
Generative Model
P(W) is constant does not affect the argmax
P(a,b) joint distribution
P(a|b) conditional distribution
P(a) marginal distribution
P(a) = Œ£b P(a,b)

Page 17:
Modeling P(T,W)
‚Ä¢ P(T,W) = P(w1,w2,w3,‚Ä¶,wn,t1,t2,t3,‚Ä¶tn)
‚Ä¢ Is there a problem with this?

Page 18:
Modeling P(T,W)
‚Ä¢ P(T,W) = P(w1,w2,w3,‚Ä¶,wn,t1,t2,t3,‚Ä¶tn)
‚Ä¢ Is there a problem with this? Curse of dimensionality
‚Ä¢ Language modeling
‚Ä¢ P(wt) requires N table values
‚Ä¢ P(wt|wt-1) requires N2 table values
‚Ä¢ P(wt|wt-1,wt-2) requires N3 table values
‚Ä¢ Many values have 0 counts (needs many tricks)
‚Ä¢ We can use Markov Assumptions
‚Ä¢ Or more generally, we use independence assumptions (conditional 
independence) to simplify the distribution to model

Page 19:
Hidden Markov Model
w
w
w
w
t
t
t
t
Transition probabilities
Emission probabilities
Markov assumption
Current value only depends on 
the immediate pass
T are called the hidden states. Usually comes from a finite set of possibilities
Ex:  t        {Noun, Adv, Adj, Verb}
P(T,W) = P(t1)P(t2|t1)P(t3|t2)P(t4|t3)P(w1|t1)P(w2|t2)P(w3|t3)P(w4|t4)
Transition probabilities
Emission probabilities
Initial state prob
What does the Markov 
assumption imply?

Page 20:
Hidden Markov Model
‚Ä¢ Defining HMM requires
‚Ä¢ 1. Starting state probability p0 = [0.7 0.3]
‚Ä¢ 2. Transition probability, Aij
‚Ä¢ 3. Emission probably, Bik
‚Ä¢ If emits discrete values
‚Ä¢ Discrete HMM
‚Ä¢ If emits continuous values
‚Ä¢ Continuous HMM
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
Question: What‚Äôs the probability of P([I eat chinese] , [N NN N]) ? 
= p0(N) * A12 * A21 * B11 * B22 * B13
= 0.7 * 0.4 * 0.5 * 0.8 * 0.45 * 0.19
N = Noun
NN = Not Noun
i,j ‚Äì state (tag) index
k ‚Äì emission (word) index

Page 21:
Hidden Markov Model
‚Ä¢ How to estimate A and B?
‚Ä¢ Counts!
P(A11) = Count(from N to N)
Count(N)
‚Ä¢ Counts with interpolation to avoid 0 counts
‚Ä¢ P(A11) = œÅ Count(from N to N)   +     (1-œÅ) Count (N)
Count(N)                             Count (words)
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
bigram
unigram
œÅ is interpolation weight

Page 22:
Hidden Markov Model
‚Ä¢ How to estimate A and B?
‚Ä¢ Counts!
P(B11) = Count(‚ÄúI‚Äù , N)
Count(N)
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45

Page 23:
Decoding
‚Ä¢ Recall we want to find the sequence of tags that 
maximizes the joint probability
‚Ä¢ argmaxT P(T,W)
‚Ä¢ How to do this?
‚Ä¢ Brute force
‚Ä¢ Find all possible sequence of T, calculate P(T,W) and compare
‚Ä¢ Length N words, B possible tags
‚Ä¢ Big O = ?
‚Ä¢ Depth first search?
‚Ä¢ Breadth first search?

Page 24:
Breadth first/depth first search
N
NN
N
NN
N
NN
N
NN
N
NN
N
NN
N
NN
I
eat
chinese

Page 25:
The Viterbi Algorithm (Dynamic 
programming)
‚Ä¢ Some computation are redundant
‚Ä¢ We can save computation from previous steps

Page 26:
Dynamic programing
‚Ä¢ Saving computation for future use. How?
‚Ä¢ Example: Find best route from A to B
A
B
c
1
5
5
2
3
4

Page 27:
Redundancy
N
NN
N
NN
N
NN
N
NN
N
NN
N
NN
N
NN
I
eat
chinese
The nodes with lower probability is redundant 

Page 28:
The Viterbi Algorithm (Dynamic 
programming)
‚Ä¢ Some computation are redundant
‚Ä¢ We can save computation from previous steps
‚Ä¢ Creates two matrices
‚Ä¢
saves the best probability at word position i for hidden state t
‚Ä¢ B[i, t] saves the previous hidden state that maximize this current 
state probability

Page 29:
Viterbi

Page 30:
Viterbi
N
NN
I
eat
chinese
N
NN
N
NN
Save only the best path till that point

Page 31:
Decoding example
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
ùùÖ[i, t]
I
eat
Chinese
State N
0.8
State NN
0.1
B[i, t] 
I
eat
Chinese
State N
-
State NN
-
We ignore the log to do easy compute
We also ignoring initial state probability.
What if we want to include it?

Page 32:
Decoding example
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
ùùÖ[i, t]
I
eat
Chinese
State N
0.8
0.005
State NN
0.1
B[i, t] 
I
eat
Chinese
State N
-
N
State NN
-
0.8 * 0.6 * 0.01 vs 0.1 * 0.5 * 0.01
0.0048              vs 0.0005

Page 33:
Decoding example
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
ùùÖ[i, t]
I
eat
Chinese
State N
0.8
0.005
0.014
State NN
0.1
0.144
0.032
B[i, t] 
I
eat
Chinese
State N
-
N
NN
State NN
-
N
NN

Page 34:
Decoding example (backtrack)
Aij
To N
To NN
From N
0.6
0.4
From 
NN
0.5
0.5
Bik
I
eat
Chinese
State N
0.8
0.01
0.19
State NN
0.1
0.45
0.45
ùùÖ[i, t]
I
eat
Chinese
State N
0.8
0.005
0.014
State NN
0.1
0.144
0.032
B[i, t] 
I
eat
Chinese
State N
-
N
NN
State NN
-
N
NN
N, NN, NN

Page 35:
Decoding (Reconstructing T)
‚Ä¢ We can find the best T by
‚Ä¢ Find that best probability at the end
‚Ä¢ Backtrack according to B[i,t]
‚Ä¢ This gives a big O of
‚Ä¢ We need to compute and create a table of size O( B N)
‚Ä¢ For each value we need to perform B computations
‚Ä¢ O( B2 N),  Space complexity of O(2 B N)
‚Ä¢ What happens if B is big?

Page 36:
Large hidden states
Very expensive
B2

Page 37:
Pruning with Beamsearch
Keep K active states at each step
K = 3

Page 38:
Pruning with Beamsearch
Keep K active states at each step
K = 3
Prune, keep K best nodes

Page 39:
Pruning with Beamsearch
Keep K active states at each step
K = 3

Page 40:
Pruning with Beamsearch
Keep K active states at each step
K = 3

Page 41:
Pruning with Beamsearch
Keep K active states at each step
K = 3

Page 42:
Pruning with Beamsearch
Keep k active states at each step
K = 3
prune

Page 43:
Pruning with Beamsearch
Keep k active states at each step
Beam size = K = 3
O(kB) per word index

Page 44:
Inadmissibility
Pruning can give the wrong answer
True answer is lost because of the pruning

Page 45:
Beam search
‚Ä¢ Beam search is inadmissible (can be wrong)
‚Ä¢ Size of beam affect the quality of the answer
‚Ä¢ Practically still useful even for small size (K < 10 in machine 
translation)
‚Ä¢ We will use beam search again for text generation.

Page 46:
HMM assumptions and disadvantages
‚Ä¢ No dependency across words
‚Ä¢ HMM is a generative model P(T, W )
‚Ä¢ But we care about P(T | W)
‚Ä¢ Mismatch between final objective and model learning objective
w
w
w
w
t
t
t
t

Page 47:
Solution: Conditional Random Fields 
(CRF)
‚Ä¢ Every point in the chain now depends on the entire 
sentence
‚Ä¢ CRF is a discriminative model
W
t
t
t
t
P(T|W) = œÄ P(tn | tn-1, W) = P( t1 | W) P( t2 | t1 W) P(t3 | t2 W) P(t4 | t3 W)
Random variable
Random/stochastic process
Random field

Page 48:
Linear chain CRF
Linear-chain CRF models with these independent 
assumption:
(1) each label tn only depends on previous label tn-1
(2) each label tn globally depends on x
48
tN-1
tN
t1
t2
t3
W
...
Problem: This is a big function (depends on n + 2 things). Hard to estimate using 
our previous method (counting)

Page 49:
Workaround
‚Ä¢ Probability distribution is a function (with special constraints)
‚Ä¢ Find a function that will represent ‚Äúprobabilities‚Äù
‚Ä¢ We can turn functions into probabilities easily.
‚Ä¢ Softmax function normalization
‚Ä¢ We just need to have a function that give higher values to more likely 
inputs

Page 50:
Goal
‚Ä¢ Find a function that will represent ‚Äúprobabilities‚Äù
‚Ä¢ Turn functions into probabilities
‚Ä¢ Softmax function normalization
‚Ä¢ We just need to have function that give higher to more likely inputs
‚Ä¢ Building functions that represent the whole sequence is 
hard
‚Ä¢ We‚Äôll build by combining pieces
‚Ä¢ But each piece should have the form
‚Ä¢ This is from our independence assumption.
‚Ä¢ We call these functions, feature functions

Page 51:
At each time step, a feature function 
is used to capture some characteristics of current label and 
the observation.
A feature function in linear-CRF:
Feature function
51
current label
previous 
label
input 
sequence
current index
returns a real value

Page 52:
Example features
In general, we often define a feature function as a binary 
function, taking current label and its dependent variable into 
account. For example:
‚Ä¢
transition function
52
n = 3
tn-1
tn
t1
t2
t3
W
...

Page 53:
Feature function: More examples
‚Ä¢
state function
‚Ä¢
The whole input sequences can be used in a feature 
function.
53
tN-1
tT
t1
t2
t3
W
...

Page 54:
Feature function: More example
Other features other than word form can be used too.
54

Page 55:
At each time step, a potential                            is a function 
that takes all feature functions into account, by summing 
their products with the associated weight 
Potential
55
number of all feature functions
kth feature function
weight associated with each 
feature function, estimated 
within training process
exponential function: used to 
convert the summation to the 
positive range
factors at time n

Page 56:
Potential: Example 
56
n
n=1
n=2
n=3
n=4
..
.
t*
NOUN
VERB
NOUN
VERB
..
.
w
The
fastest
fox
jumps
..
.
From feature functions and trained 
weights on the right, we can 
compute potentials for the 
predicted label sequence t* at 
time step n=3 as following:

Page 57:
Probability of the whole sequence
57
tN-1
tN
t1
t2
t3
W
...
<s>
</s>
Œ®1
Œ®N
Œ®2
Œ®3
Œ®N-1
x
x
x
x
x
...
Sum of scores for all possible 
labels with all possible input 
sequences
Score of a sequence label T for 
a given sequence input W
Joint probability distribution of input and output sequence             
can be represented as:
With p(T, W) we can compare
and pick the best T

Page 58:
Special states and characters
To simplify modeling, we add two new special states and 
characters:
<s> indicates the beginning of the sequence
</s> indicates the end of the sequence
tN-1
tN
t1
t2
t3
W
...
<s>
</s>

Page 59:
From the definition of factors, the joint distribution can be 
represented by
Product of sum over feature functions
59
Computing Z is intractable:
Imagine a sentence of 20 words with vocabulary size of 
100,000
we have to consider all (100000)20 possible input 
sequences!

Page 60:
Linear-chain CRF
Modeling conditional probability distribution    P(T|W)    is 
enough for classification tasks.
So, in linear-chain CRF, we model the conditional distribution 
by using these two equations:
60

Page 61:
Linear-chain CRF
A linear-chain CRF is a conditional distribution 
, where Z(x) is an instance-specific normalization function
61

Page 62:
Linear-chain CRF
62
normalization 
function
the sum of 
products of all 
possible output 
sequences
not the same as Z in 
the joint distribution
sum of weighted 
feature functions at 
one time step, then 
taken to the 
exponential function
multiply 
over 
all time 
steps

Page 63:
Linear-chain CRF big picture
‚Ä¢ Wants P(T|W)
‚Ä¢ Assumes independence, where we only consider P(tt-1,tt,W)
‚Ä¢ How to model P(tt-1,tt,W)?
‚Ä¢ Still too hard, let‚Äôs make it into a function where high value means 
high probability ‚Äì potential functions
‚Ä¢ Still too hard, let‚Äôs build it from pieces ‚Äì feature functions
‚Ä¢ We can get P(T,W) by multiplying all 
‚Ä¢ This is not a probability, need a normalization
‚Ä¢ We can also get P(T|W) from multiplying all                  
‚Ä¢ Still need a normalization, but easier.
‚Ä¢ This is our model, but!
‚Ä¢ How to inference? How to train? What features functions?

Page 64:
How to inference?
‚Ä¢ If we are given the model, and W
‚Ä¢ Find T
‚Ä¢ Not so straight forward, many possible T
‚Ä¢ Noun, adjective, verb
‚Ä¢ Noun, noun, verb
‚Ä¢ Verb, noun, noun
‚Ä¢ Too many possibilities to compare
‚Ä¢ Solution: Dynamic programming, just like HMM

Page 65:
Viterbi algorithm
Viterbi algorithm is an algorithm for decoding based on 
dynamic programming.
From the equation, we can see the Z(W) is the same for all 
possible label sequences, so we can consider only the part 
in the rectangle 
65
Find the label 
sequence T that 
maximize this value
Goes over time step just like HMM viterbi

Page 66:
Viterbi: Structure 
Create two 2D arrays: VTB and Var
66
ADJ
ADP
...
<S>
</S
>
0
1
...
N
N+1
ADJ
ADP
...
<S>
</S
>
0
1
...
N
N+1
VTB(<s>, T) stores the highest 
value a label sequence that yT is 
<s> can take 
Var(<s>, T) stores the label yT-1
in the sequence that yields the 
value in VTB(<s>, T)

Page 67:
Viterbi: Initialization
67
0
0.12
0
0.03
...
...
0
10e-
9
1
10e-
3
ADJ
ADP
...
</S
>
<S>
0
1
...
N
N+1
ADJ
ADP
...
</S
>
<S>
0
1
...
N
N+1
-
<s>
-
<s>
...
...
-
<s>
-
<s>
The first label of the output 
sequence must be <s>
VTB
Var
For all other labels, 
apply the same way 
as ADJ
Factors at time t=1, for 
current label=ADJ, 
previous label=<s>
1

Page 68:
Viterbi: Iteration
68
0
0.12
...
2.32
0.22
0
0.03
...
0.02
0.10
...
...
...
...
...
0
10e-
9
...
0.03
3.09
1
10e-
3
...
1.12
0.02
ADJ
ADP
...
</S
>
<S>
0
1
...
N
N+1
ADJ
ADP
...
</S
>
<S>
0
1
...
N
N+1
-
<s>
...
NOUN
PREPO
-
<s>
...
NOUN
ADJ
...
...
...
...
...
-
<s>
...
VERB
ADJ
-
<s>
...
X
X
Var(t, n) Stores the value i
that maximize the value of 
VTB(t,n)
VTB
Var
Factors at 
time n
Maximum value 
that label i can take 
at time n-1
Iterate from n=2 to n=N+1
Find the max 
among values 
from all 
previous label i

Page 69:
Backtrack from Var(</s>, N+1) to get the label sequences 
that maximize P(T|W)
Viterbi: Finalize
ADJ
ADP
...
</S
>
<S>
0
1
...
T
N+1
-
<s>
...
NOUN
PREPO
-
<s>
...
NOUN
ADJ
...
...
...
...
...
-
<s>
...
VERB
ADJ
-
<s>
...
X
X
Var
For example:
output sequence = 
<s>, NOUN, ‚Ä¶, NOUN, ADJ, </s>

Page 70:
Linear-chain CRF big picture
‚Ä¢ Wants P(T|W)
‚Ä¢ Assumes independence, where we only consider P(tt-1,tt,W)
‚Ä¢ How to model P(tt-1,tt,W)?
‚Ä¢ Still too hard, let‚Äôs make it into a function where high value means 
high probability ‚Äì potential functions
‚Ä¢ Still too hard, let‚Äôs build it from pieces ‚Äì feature functions
‚Ä¢ We can get P(T,W) by multiplying all 
‚Ä¢ This is not a probability, need a normalization
‚Ä¢ We can also get P(T|W) from multiplying all                    and 
use chain rule.
‚Ä¢ Still need a normalization, but easier.
‚Ä¢ This is our model, but!
‚Ä¢ How to inference? Viterbi
‚Ä¢ How to train? What features functions?

Page 71:
Training Parameters?
Parameters to be learned are weights associated to each 
feature functions. So, the number of parameters equals the 
number of feature functions.
71
parameters

Page 72:
Training objective
For linear-chain CRF, parameters are trained by maximum 
likelihood.
To clarify, parameters Œ∏ are trained to maximize the log 
probability of all pairs of label T(i) and input W(i) in the 
training set. (i) represents the ith training sentence.
72
(maximize)

Page 73:
Learning algorithm
To learn parameters from the loss function          , several 
learning algorithm can be used. Some popular learning 
algorithms for linear-chain CRFs are
‚Ä¢ Limited-memory BFGS
‚Ä¢ Stochastic Gradient Descent
73

Page 74:
Feature functions?
‚Ä¢ Anything you can think of, the more the better.
‚Ä¢ The model will learn what is important.

Page 75:
CRFsuite
‚Ä¢
An implementation of CRFs for labeling sequential data in 
C++
SWIG API is provided to be an interface for various 
languages
http://www.chokkan.org/software/crfsuite/
‚Ä¢
python-crfsuite: Python binding for crfsuite
https://github.com/scrapinghub/python-crfsuite
‚Ä¢
An example use of python-crfsuite can be found at 
https://github.com/scrapinghub/python-
crfsuite/blob/master/examples/CoNLL%202002.ipynb
75

Page 76:
CRF with neural networks
‚Ä¢ Change the softmax layer and loss function
‚Ä¢ CRFlayer: P(y|yt-1,x) not just P(yt|x) 
Typical softmax only considers current word label
not the sequence

Page 77:
Neural network for POS
GRU
GRU
GRU
GRU
embedding
embedding
embedding
embedding
Fully 
connect
Fully 
connect
Fully 
connect
Fully 
connect
Softmax
Softmax
Softmax
Softmax

Page 78:
Neural network for POS with CRF output
GRU
GRU
GRU
GRU
embedding
embedding
embedding
embedding
Fully 
connect
Fully 
connect
Fully 
connect
Fully 
connect
Linear chain CRF
hn(tn|W)
h1(t1|W)
h2(t2|W)
h3(t3|W)
h4(t4|W)
maximize likelihood of sequence of tags instead

Page 79:
Neural network for POS with CRF output
Fully 
connect
Fully 
connect
Fully 
connect
Fully 
connect
[ hn(tn|W)T(tn|tn-1)]
h1(t1|W)
h2(t2|W,t1)
h3(t3|W,t2)
h4(t4|W,t3)
Trans
ition 
score
Trans
ition 
score
Trans
ition 
score
Additional param with 
size of |T|^2
a slight problem in gradient computation
needs to use forward-backward algorithm to compute the gradient 
in this layer for efficient computation see Appendix A in the book

Page 80:
Neural network for POS with CRF output
‚Ä¢ Need to use Viterbi for finding the best sequence
‚Ä¢ Or instead of using the full sequence when decoding, consider each 
time step instead (marginal inference ‚Äì faster decoding)
‚Ä¢ This is pretty much a regular softmax during decoding
‚Ä¢ Loss function: computed likelihood as a sequence
‚Ä¢ Loss = -log(P(T*|W)) where T* is the true output
Example code: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html
[ hn(tn|W)T(tn|tn-1)]

Page 81:
Performance
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF
https://arxiv.org/pdf/1603.01354.pdf

Page 82:
Thai Nested NER
https://aclanthology.org/2022.findings-acl.116/
https://github.com/vistec-AI/Thai-NNER
1 model per layer
1 model all layer

Page 83:
BERT and PoS

Page 84:
Additional reading
‚Ä¢
https://web.stanford.edu/~jurafsky/slp3/
‚Ä¢
Chap 17 (PoS)
‚Ä¢
HMM (Appendix A)
‚Ä¢
Huggingface‚Äôs token classification tutorial
https://huggingface.co/docs/transformers/en/tasks/token_c
lassification

Page 85:
Conclusion
‚Ä¢ What is token classification?
‚Ä¢ Traditional methods
‚Ä¢ Sequence methods
‚Ä¢ HMM
‚Ä¢ CRF
‚Ä¢ Viterbi and beamsearch
‚Ä¢ Neural network methods

