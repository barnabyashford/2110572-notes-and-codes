Page 1:
Large Language Model
1

Page 2:
Outline
‚Ä¢
Generative AI
‚Ä¢
Emergent properties & scaling laws
‚Ä¢
Alignment
‚Ä¢
Instruction tuning
‚Ä¢
Preference learning
2

Page 3:
What is generative AI?
‚óèPredictive AI
‚óãpredicts
‚óãclassify
https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1/
3

Page 4:
https://knowyourmeme.com/memes/ai-will-smith-eating-spaghetti
What is generative AI?
‚óèGenerates
‚óã
Creative tasks that was believe to be hard for AI a couple years 
ago
https://x.com/jerrod_lew/status/1868809004400754871
https://www.tiktok.com/@willsmith/video/7337480720115371295?lang=en

Page 5:
Generative modeling
‚Ä¢
Learns P(X,Y) or P(X|Y)
‚Ä¢
Can sample (generates) X from P(X|Y)
‚Ä¢
Y is the controlling parameter
‚Ä¢
What is P(X|Y)?
‚Ä¢
In the past, it‚Äôs often assumed to be a parametric distribution
5

Page 6:
Enter deep generative models
‚Ä¢
Distribution learning landscape
https://arxiv.org/pdf/1701.00160.pdf
https://yang-song.net/blog/2021/score/
Explicit likelihood 
maximization of P(X)
Some other way besides max P(X)
Diffusion/score-based model
6

Page 7:
Text generation
‚Ä¢
Unlike other generative tasks, turns out that explicit 
density methods (autoregressive prediction) works well 
for text
‚Ä¢
There are attempts to use GANs/RLs/Diffusion/VAE for 
text generation but they are not as competitive against 
simple cross entropy based prediction
‚Ä¢
Scales well with very large data
‚Ä¢
Recent research starts looking into ways to use diffusion/VAE 
in text generation
7

Page 8:
Pretrained Foundation Models/LLM
‚Ä¢
With self-supervised learning, Ex. contrastive learning, 
next token prediction, mask token prediction
‚Ä¢
People found that large model produces better results
LLaMA: Open and Efficient Foundation Language Models https://arxiv.org/abs/2302.13971
8
https://arxiv.org/pdf/2402.08797

Page 9:
Emergent abilities
‚Ä¢
People have found that models perform some 
capabilities much better after a certain size
9
Emergent Abilities of Large Language Models https://arxiv.org/abs/2206.07682

Page 10:
Why large model matters?
‚Ä¢
In GPT3 paper, OpenAI found that large models are 
good few-shot learners
https://arxiv.org/abs/2005.14165
Tradition Fine-Tuning (BERT)
In-Context Learning (GPT-3)
Zero-Shot
One-Shot
Few-Shot
In-Context Learning (GPT-3)
Model
‚óè
No Fine-Tune
Prompt
‚óè
Task Description
‚óè
Example
‚óè
Prompt
10

Page 11:
Why large model matters
Text Input
Prompt
Dataset: BoolQ
Zero-Shot
https://arxiv.org/abs/2005.14165
11

Page 12:
Why large model matters
On par with 
BERT-Large
https://arxiv.org/abs/2005.14165
12

Page 13:
Why large model matters
General Model: 
On average, the 
accuracy in most tasks 
are not as good
Specific Model:
Fine-tune model 
towards a specific task
Fine-Tune
https://arxiv.org/abs/2005.14165
13

Page 14:
Scaling is all you need
‚Ä¢
This realization of scale leads to several efforts to try to 
understand the relationship between data, compute, 
and performance.
14

Page 15:
Kaplan Scaling law
‚Ä¢
In 2020, OpenAI release the first paper discussing the 
‚Äúoptimal model size‚Äù 
‚Ä¢
An empirical study, think of Scaling law as Moore‚Äôs law, just an 
observation, on the log scale
‚Ä¢
Performance depends mostly on scale rather than other factors 
such as width or depth of network
‚Ä¢
Need to scale data, compute, and network size in tandem
15
Scaling Laws for Neural Language Models https://arxiv.org/pdf/2001.08361

Page 16:
Kaplan Scaling law
‚Ä¢
Large model are more sample-efficient.
‚Ä¢
Less gradient updates to reach the same performance.
‚Ä¢
Models converge slowly
‚Ä¢
Stop when you just want to stop. Late iterations give less gains.
16

Page 17:
Kaplan Scaling law
‚Ä¢
They curved fit to the data and got some dubious 
equations
‚Ä¢
In simple implications, for GPT-3, you need 1.7 text tokens per 
parameter in you LLM to be efficient
‚Ä¢
Often refer to as Kaplan scaling law (the first author)
17
Relationship with infinite size model
Relationship with infinite size data

Page 18:
Chinchilla Scaling law
‚Ä¢
In 2022, Deepmind 
published another scaling 
law paper
‚Ä¢
Found that model size and 
training data should be 
scaled equally. Around 20 
tokens per parameter.
‚Ä¢
Used smaller model, but 
trained with more data to 
get better performance
18
https://arxiv.org/abs/2203.15556

Page 19:
Chinchilla Scaling Law
‚Ä¢
They offer a method to think about the data size D and 
model size N that will gives compute optimal (given a 
fixed compute budget) as
19
What happens with a finite model
What happens with a finite data
Inherent loss (noise)
With experiments from OpenAI, A = 406.4, 
B = 410.7, E = 1.69
Alpha = 0.34, Beta = 0.25
If we plug in gopher numbers, L = 1.993
Chinchilla L = 1.936

Page 20:
Other Scaling Laws
‚Ä¢
Broken Neural Scaling laws 2023 
https://arxiv.org/abs/2210.14891
‚Ä¢
A work offering scaling laws that cover various tasks (besides 
NLP)
‚Ä¢
The laws so far are for training time, we can also 
investigate inference time scaling (test-time scaling)
20

Page 21:
Outline
‚Ä¢
Generative AI
‚Ä¢
Emergent properties & scaling laws
‚Ä¢
Alignment
‚Ä¢
Instruction tuning
‚Ä¢
Preference learning
21

Page 22:
Alignment
22

Page 23:
Back to prompting
Prompt Design
Input Text:A great movie!!! 
Prompt:
Q: Is it positive or negative? 
A: <blank>
Freeze
Pre-trained 
Model
Freeze
Positive
Input to model
Finetuning
Input data: A great movie!!! -> Positive
Waste of time -> Negative
Pre-trained 
Model
Finetune
Finetuned
Model
Input data: Fabulous
Positive
23

Page 24:
Verbalization and prompt design
Different Template
Different Verbalizer 
https://arxiv.org/pdf/2104.08691.pdf
24

Page 25:
Why do we even need 
Prompting?
25

Page 26:
Self-supervised models are 
stochastic parots
Input (Prompt)
Output
The patient was died. <fill next‚Ä¶>
The patient's body was found in a dark alley 
behind the hospital‚Äôs‚Ä¶
‚ÄúThe patient was died.‚Äù correct this 
<fill next‚Ä¶>
claim if you really believe such figures‚Ä¶.
Poor English input: The patient was 
died. <fill next‚Ä¶>
Good English output: The patient died.
26

Page 27:
Alignment
‚Ä¢
It‚Äôs hard to make a self-superivsed model do what we 
want. This is called the alignment problem (aligns model 
capabilities with users‚Äô interests). Ultimately, you will 
need supervision for this!
‚Ä¢
Current solutions
‚Ä¢
Prompt engineer (manual)
‚Ä¢
Low parameter finetuning ‚Äì LoRa, mixture of experts, learned 
prompts, etc.
‚Ä¢
Preference learning
https://www.alignmentforum.org/
Web forum discussing the alignment problem
27

Page 28:
InstructGPT
‚Ä¢
In Mar 2022 (ChatGPT came out Dec 2022), OpenAI 
released a paper InstructGPT
28
https://arxiv.org/pdf/2203.02155

Page 29:
InstructGPT
‚Ä¢
Step 0: pretrain a language model eg GPT3
‚Ä¢
Step 1: Supervised fine-tuning (SFT)
‚Ä¢
Step 2: Reward Model training
‚Ä¢
Step 3: Reinforcement Learning with Human feedback 
(RLHF)
29
https://arxiv.org/pdf/2203.02155

Page 30:
Supervised Finetuning/Instruction 
Tuning
‚Ä¢
Create a dataset of questions and 
answers.
‚Ä¢
Often called ‚ÄúInstruction data‚Äù
30

Page 31:
Instruction data
‚Ä¢
Just example Q A pairs
‚Ä¢
Alpaca is one of the 
earliest open instruction 
data. They just use 
ChatGPT to generate 
the answers.
‚Ä¢
A kind of distillation
‚Ä¢
Easier and faster than 
hiring a bunch of humans. 
People can copy you! 
OpenAI terms has a non-
compete clause.
‚Ä¢
Deepseek uses 1.5 
million
31
https://huggingface.co/datasets/iamketan25/al
paca-instructions-dataset
https://github.com/tatsu-lab/stanford_alpaca

Page 32:
Self-instruct
‚Ä¢
A technique to use LLM to generate data to train more 
LLMs!
32
https://arxiv.org/pdf/2212.10560

Page 33:
Thai instruction data
‚Ä¢
Still super small, most are automatically generated
‚Ä¢
OpenThaiGPT
33
https://openthaigpt.aieat.or.th/previous-versions-and-resources/released-datasets-14-04-23

Page 34:
Thai instruction data
‚Ä¢
WangchangX
34
https://huggingface.co/datasets/airesearch/wa
ngchanx-seed-free-synthetic-instruct-thai-
120k
https://huggingface.co/datasets/airese
arch/WangchanThaiInstruct_7.24

Page 35:
Supervised Finetuning/Instruction 
Tuning
‚Ä¢
Create a dataset of questions and 
answers.
‚Ä¢
Often called ‚ÄúInstruction data‚Äù
‚Ä¢
Trained using next word prediction 
on instruction data.
35

Page 36:
InstructGPT
‚Ä¢
Step 0: pretrain a language model eg GPT3
‚Ä¢
Step 1: Supervised fine-tuning (SFT)
‚Ä¢
Step 2: Reward Model training
‚Ä¢
Step 3: Reinforcement Learning with Human feedback 
(RLHF)
36
https://arxiv.org/pdf/2203.02155

Page 37:
37

Page 38:
Introduction to RL
38

Page 39:
RL problem
Observations
Actions
Environment
Agent
Reward

Page 40:
3 Modes of Learning
Supervised Learning
Reinforcement Learning
Unsupervised Learning

Page 41:
3 Modes of Learning
Reinforcement Learning (RL)
‚óèObserve:  
‚óãThe states (x1, x2, x3, ‚Ä¶ )
‚óãThe reward ( r1, r2, r3, ‚Ä¶ ) 
‚óèCan also take actions
‚óãa1 , a2 , a3 , ‚Ä¶ 
‚óèWhat are the best actions? 
‚óãSuch that we will receive 
highest accumulative 
rewards

Page 42:
Reward (rt)
State (st) 
Action (at)
RL framework

Page 43:
Rewards-based learning
‚Ä¢ Maximise the rewards
‚Ä¢ Can we design any desired
behaviour with reward?
Rt = Œîdistance
Rt = score
, win
, lose

Page 44:
Policy
‚Ä¢ Policy = a mapping from a state to an action
‚Ä¢ Objective of RL is to find the ‚Äò‚Äôoptimal‚Äô‚Äô policy! 
Can be either 
deterministic or 
stochastic 

Page 45:
Policy
Example: tabular policy
+1
-1

Page 46:
Policy
‚Ä¢
Example: policy to play an arcade game
46

Page 47:
Return (Cumulative rewards)
‚Ä¢ Return =  cumulative rewards with discount
rt = 0
rt+10 = 1
rt+34 = -1

Page 48:
What is learning?
‚Ä¢
Use data to find/search for the best policy 
What is the best policy?
‚Ä¢
Policy that give us the highest expected return! 

Page 49:
Two main ways to find the best 
policy 
‚Ä¢
Q-learning (Value-based)
‚Ä¢
Policy gradient (Policy-based)
49

Page 50:
Q-learning algorithm
‚Ä¢
Let‚Äôs define a state value as
‚Ä¢
Expectation of the return after visit s and follow ùõë
‚Ä¢
Let‚Äôs define a state-action value (Q-value) as
‚Ä¢
Expectation of the return after visit a state s, take action a

Page 51:
Q-learning algorithm
‚Ä¢
There exist an optimal value function associate with an 
optimal policy, 
‚Ä¢
The optimal policy is the policy that achieves the 
highest value for every state

Page 52:
Q-learning algorithm
‚Ä¢
It follows that 
‚Ä¢
and ..
‚Ä¢
Optimal actions can be found indirectly through Q-value 

Page 53:
Policy gradient
Q Learning
-
policy is implicit
-
if we already have Q, we have policy
-
we just look at Q to get 
-
Used in AlphaGo
Policy gradient
-
learns      directly explicitly 
-
use Q, V as a helper for learning
-
Used in pretty much everything else 

Page 54:
Policy gradient
‚óèUse Function Approximator to represent policy directly
S
a
S
P(a1| s)
P(a2| s)
P(a3| s)

Page 55:
Loss function for policy gradient
‚Ä¢
Start-state objective  
‚Ä¢
Average-reward objective
* d is a stationary distribution of a Markov chain.
ùûπ

Page 56:
Policy gradient
Let‚Äôs start from the average-reward objective
For simplicity let‚Äôs assume d(s) does not depend on
Almost there...

Page 57:
Policy gradient
REINFORCE trick! 
We get this by sampling a playthrough using current policy (on-policy)

Page 58:
Policy gradient theorem
There is a theorem‚Ä¶called policy gradient theorem
say that we can replace                 with 

Page 59:
What is the gradient doing?
Goal maximize rewards
Push ùùøtowards directions of higher Q(s,a)
Q(1,1) = 5  
Q(1,2) = 2
‚ñøùùø(1,2) will have a higher weight. Policy gets push towards 
action 2 

Page 60:
Notes on Policy gradient
Also known as REINFORCE or likelihood ratio.
Used by other ML fields when original loss is not 
differentiable (-Q in this case). Push network to produce 
lower loss.
Example of non-differentiable functions
argmax
(not maxpool)
sampling
Many metrics such as accuracy, preference (ranking)

Page 61:
Baselines
has high variance
is very noisy
Slow down training a lot! 
We need to reduce variance  to speed up the training

Page 62:
Baselines reduce variance
What is a good b(s)?
is a convenient choice

Page 63:
Advantage Function
When we use V(s) as baseline:
We call this the advantage function. 
It tells the relative value of the actions.
Lower variance than using absolute value of the actions.
Also called A2C

Page 64:
Noisy updates
‚Ä¢
In RL, the updates can be noisy. Our learning is based 
on Q, V, and A and might not be an accurate estimate if 
the policy changes too much.
‚Ä¢
Limit the update to a certain size? Use KL divergence 
that new policy shouldn‚Äôt be that different from old policy
64
Thus, we want to make 1-e <        < 1+e 
(clipping the value)

Page 65:
Proximal Policy Optimization (PPO)
‚Ä¢
PPO is just a policy gradient update with Advantage 
function and clipping the value so that the model does 
not move outside of the trusted region
‚Ä¢
The min is used as a pessimistic bound between no 
clipping and clipped values.
‚Ä¢
PPO is one of the defaults techniques used by OpenAI
65
https://arxiv.org/pdf/1707.06347

Page 66:
Recap PPO and RL
‚Ä¢
RL uses a reward function to learn to optimize our 
model.
‚Ä¢
Key ingredient: environment (game = input+output) and a way 
to give reward (scores from output)
‚Ä¢
PPO is a policy gradient method with tricks.
‚Ä¢
Key: what moves give me higher reward than before, try to 
change the gradients so that the move is more likely
66

Page 67:
InstructGPT
‚Ä¢
Step 0: pretrain a language model eg GPT3
‚Ä¢
Step 1: Supervised fine-tuning (SFT)
‚Ä¢
Step 2: Reward Model training
‚Ä¢
Step 3: Reinforcement Learning with Human feedback 
(RLHF)
67
https://arxiv.org/pdf/2203.02155

Page 68:
Reward model
‚Ä¢
We need a way to give 
rewards for any possible 
input+output pairs.
‚Ä¢
Technically can be a metric we 
learned last week, but we talked 
about how human judgement is 
still the best
‚Ä¢
We also talked about training a 
model to mimic human 
judgement, and how chatGPT 
can gives scores
‚Ä¢
Collect data to train a reward 
model.
‚Ä¢
OpenAI uses GPT -> finetune -> 
reward model
68

Page 69:
‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏ô‡∏¥‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ä‡∏≠‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ‡∏Å‡∏±‡∏ô 
‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡πâ‡∏≤‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏à‡∏±‡∏ö‡∏™‡∏•‡∏≤‡∏Å‡∏î‡∏π‡∏™‡∏¥‡∏Ñ‡∏∞‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏∏‡πä‡∏Å‡∏ï‡∏≤ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πà‡∏ô 
‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤ ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö ‡∏Ø‡∏•‡∏Ø ‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡πÅ‡∏•‡∏∞
‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏Ñ‡πà‡∏∞
[‡∏á‡∏ö 400-500 ‡∏ö‡∏≤‡∏ó] ‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô ‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡∏ß‡∏±‡∏ç‡∏à‡∏±‡∏ö‡∏â‡∏•‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô 
‡∏â‡∏•‡∏≤‡∏Å‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß ‡∏â‡∏•‡∏≤‡∏Å‡πÄ‡∏á‡∏¥‡∏ô ‡∏â‡∏•‡∏≤‡∏Å‡πÅ‡∏Ç‡πá‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏â‡∏•‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ
‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏â‡∏•‡∏≤‡∏Å‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°
‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™ ‡∏≤‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î
‡∏â‡∏•‡∏≤‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏µ‡∏™‡∏±‡∏ô‡∏™‡∏î‡πÉ‡∏™ ‡πÅ‡∏•‡∏∞‡∏â‡∏•‡∏≤‡∏Å‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°
‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
Q: ‡∏Ç‡∏≠‡∏Ñ ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô ‡∏≥‡∏Ñ‡∏∞ ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ç‡∏ß‡∏±‡∏ç‡∏à‡∏±‡∏ö‡∏â‡∏•‡∏≥‡∏Å‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ ‡∏á‡∏ö 400-500 ‡∏ö‡∏≥‡∏ó‡∏Ñ‡πà‡∏∞
A
B
Preference data
69
Preference data needs to be collected for the reward model

Page 70:
Preference data
‚Ä¢
LLAMA ‚Äì ask annotators to write a prompt -> two 
sampled model response -> label them (‚Äúchosen‚Äù and 
‚Äúrejected‚Äù)
‚Ä¢
LLAMA3 added a ‚Äúedited‚Äù response, where chosen is further 
editted
70
https://ai.meta.com/research/publications/the-llama-3-herd-of-models/

Page 71:
Reward model training
‚Ä¢
Many possible settings
‚Ä¢
Contrastive loss
‚Ä¢
R( x, y_chosen) > R( x, y_rejected)
‚Ä¢
Predict some score value
‚Ä¢
Use a heuristic
‚Ä¢
Deepseek R1 is trained on code and math problems.
‚Ä¢
Scores can be checked against the real solution.
71

Page 72:
Reinforcement learning with human 
feedback (RLHF)
‚Ä¢
The Reward model is used to 
trained with PPO
‚Ä¢
They also add a per-token KL 
penalty from the SFT model so that 
the optimized model does not go 
too far away from the SFT model
72

Page 73:
Reinforcement learning with human 
feedback (RLHF)
‚Ä¢
The Reward model is used to 
trained with PPO
‚Ä¢
But, let‚Äôs think again why do we 
need RL?
‚Ä¢
This task is hard to do supervised 
learning
‚Ä¢
Cannot think of all possible answer 
and scoring for creative tasks
‚Ä¢
RL offers a scalable solution that can 
covers multiple domain
‚Ä¢
Can discover new moves
‚Ä¢
‚Äúmove 37‚Äù
‚Ä¢
Deepseek aha moment
‚Ä¢
Drawbacks: sophisticated framework. 
Hard to train
73

Page 74:
Deepseek v3 aha moment
‚Ä¢
Probably won‚Äôt show up in supervised data
74
https://arxiv.org/abs/2501.12948

Page 75:
Notes on LLM
‚Ä¢
From what we learned so far
‚Ä¢
LLM is autoregressive
‚Ä¢
Errors cascade
‚Ä¢
LLM is just next word prediction trained on trillions of tokens
‚Ä¢
Likes most probable next token, with some randomness mixed in
‚Ä¢
Trained to reward human preference
‚Ä¢
Can you see why hallucination is pretty much inherent 
in LLMs?
‚Ä¢
Backtracking might be an important capability for LLMs 
to do well in this setting.
75

Page 76:
Other preference 
learning technique 
76

Page 77:
History
‚Ä¢
In 2023-early 2024, people thinks that RLHF seems to 
be the secret sauce to make ChatGPT work.
‚Ä¢
Many people proposes alternatives to PPO
‚Ä¢
DPO
‚Ä¢
ORPO
‚Ä¢
GRPO
77

Page 78:
Direct Preference Optimization 
(DPO)
‚Ä¢
Skips reward model training
‚Ä¢
Learns directly from the preference data
78
https://arxiv.org/pdf/2305.18290

Page 79:
Why does DPO work?
‚Ä¢
DPO is just a derived version of PPO assuming binary 
choice in the reward model.
79
A2C update
Assuming binary reward model

Page 80:
Odds Ratio Preference 
Optimization (ORPO)
‚Ä¢
DPO requires you to train SFT then do DPO
‚Ä¢
ORPO tries to do this in one step
80
https://arxiv.org/pdf/2403.07691
Odds(x) = k means x is k times
more likely than not 

Page 81:
ORPO
‚Ä¢
Note: although they claimed 
that ORPO doesn‚Äôt require 
separate SFT step, in 
practice people found that it 
is beneficial to do SFT than 
ORPO
81
Odds(x) = k means x is k times
more likely than not 
Oddsratio = how much more likely to generated a 
chosen than rejected

Page 82:
Problems with fixed preference 
data.
‚Ä¢
While ORPO and DPO works well on paper, they are 
optimized on the SFT or Preference data only. This 
might cause the model to overfit to the data.
‚Ä¢
RL is still required to generalize
‚Ä¢
Or the preference data needs to keep being updated over 
iterations just like in LLAMA 3.1 or Deepseek
82

Page 83:
Group Relative Policy Optimization 
(GRPO)
‚Ä¢
GRPO is a version of PPO which calculates the 
advantage function in a different manner
83
b(s) is typically the Value function V(s)
In this framework, the policy and value model is learned
Value model is cumbersome to learn because it‚Äôs the 
same architecture as the base model.
Also, V(s) might not be a good choice for b(s) because 
it might be hard to estimate in the LLM context.
https://arxiv.org/pdf/2402.03300

Page 84:
GRPO
84
A is averaged over the different outputs from the same input
(We need b(s))

Page 85:
When Policy Gradient is just 
supervised learning
85
The only difference is where the data comes from

Page 86:
Why RL works?
‚Ä¢
In DeepSeekMath, they found that models trained with 
RL perform better than SFT models in ‚Äúalignment‚Äù
‚Ä¢
Top outputs have higher scores but capabilities still remain the 
same.
‚Ä¢
Bottlenecks
‚Ä¢
Rewards learning ‚Äì how to get nice reward model for 
subjective tasks
‚Ä¢
Data - sampling of questions and answers for the model to 
learn
‚Ä¢
Algorithm ‚Äì most RL algorithms are based on always true 
rewards
86
https://arxiv.org/pdf/2402.03300

Page 87:
Summary
‚Ä¢
Scaling laws
‚Ä¢
ChatGPT training
‚Ä¢
Pre-training
‚Ä¢
Post-training
‚Ä¢
SFT
‚Ä¢
Preference optimization
87

Page 88:
Midterm
‚Ä¢
In class. Room will be announced later.
‚Ä¢
1 page A4 front back
‚Ä¢
Calculator
Part A) Aj.Peerapon (25 points)
- Tokenization
- LM + subwords
- Attention + transformer
- Text gen + eval measure
Part B) Aj.Ekapol (25 points)
- Deep learning/Pytorch
- Word representation (sparse, dense)
- PoS (HMM)
- Sentence Embedding/Text classification
- Open question (design)
88

Page 89:
Takehome
89

Page 90:
Project
Group of 2-5 people
We expect more from 
more people
Anything text/NLP 
related
Must has some 
application component 
(cannot be purely basic 
NLP task)

Page 91:
HOW TO READ A 
SCIENTIFIC ARTICLE

Page 92:
2 Paper types
‚óèReview article/tutorial
‚óãGive insights about the field
‚óãUseful for learning about a new field
‚óãRead multiple to avoid the author‚Äôs bias
‚óãTitle usually has ‚Äúreview‚Äù or ‚Äútutorial‚Äù
‚óèPrimary research article
‚óãMore details on the experiments and results

Page 93:
Parts of an article
‚óèAbstract
‚óèIntroduction
‚óèMethods
‚óèResults and discussion
‚óèConclusion
‚óèReference

Page 94:
Things to look for before reading an 
article
‚óèPublication date
‚óèAuthor names
‚óãPrevious and newer publications
‚óèKeywords
‚óèAcknowledgements and funding sources
https://arxiv.org/pdf/2403.07691

Page 95:
Getting the big picture
‚óèRead the abstract
‚óèRead the introduction
‚óãWhat is the research question?
‚óãWhat is the method?
‚óãWhat had been done? How is it different from other work?
‚óèLook at figures and results
Tip: keep track of terms you don‚Äôt understand

Page 96:
First reading
‚óèReread the introduction
‚óèSkim methods
‚óèRead results and discussion
‚óãDoes the figures make sense now?
‚óèWrite on the article!

Page 97:
Understanding the article
‚óèReread the article (until you get what you want)
‚óèCheck references for parts you don‚Äôt understand
‚óèReread the abstract
‚óãDoes your understanding match the abstract?
‚óèNote down important points. This might come in handy 
when you write you paper/thesis!

Page 98:
Evaluating the article
‚óèDoes the method make sense?
‚óãWhat are the limitations that the authors mention?
‚óãAre there other limitations?
‚óãCan it be used in other situations?
‚óèAre the experiments legitimate?
‚óãThe sample size is big enough? How big?
‚óãWhat kind of dataset is used? Hard enough to current standards? 
Baselines?
‚óãThe evaluation criterion is sound?
‚óèHave these results been reproduced?
‚óãLook for articles that cite this paper

Page 99:
ML paper checklist
‚óèWhat is being done?
‚óèHow is it being done?
‚óãHow is it different from previous work
‚óèWhat is the dataset?
‚óãNature of dataset. How many training/testing samples? How many 
classes/vocab size? Etc.
‚óèEvaluation
‚óãWhat are the baselines? Metrics is okay?
‚óèPracticality/limitations
‚óãProne to parameter tuning?
‚óãComputing resource / Runtime (training and testing)
‚óãOther assumptions? Supervised, unsupervised, etc.

Page 100:
Useful tools
‚óèhttps://scholar.google.com
‚óãFor finding other articles by the same authors or paper that cites 
the article
‚óãDeep research of various kinds are quite useful for 
researching a topic. Be careful of hallucinations even if it 
has citation!
‚óãMany famous papers has web presence
reddit, twitter, or openreview
video explaining the paper

Page 101:
Paper presentation
Pick from the list of papers we provide (right after midterm 
exam)
8 minute presentation + 2 minute progress update + 2 
minute QA
Should cover the ML paper checklist

