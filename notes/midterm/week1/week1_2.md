Page 1:
Dictionary-based Tokenization
2110572: NLP SYS
Assoc. Prof. Peerapon Vateekul, Ph.D.
Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University
peerapon.v@chula.ac.th
Credits to: TA.Pluem, TA.Knight, and all TA alumni 
Based on Aj.Ekapolâ€™s slide in 2017

Page 2:
Outlines
The need for segmentation
Thai Tokenization
1) Longest Matching
2) Maximal Matching
2

Page 3:
The need for segmentation
â—Text as a stream of characters
â—We need a way to understand the meaning of text
â—‹Break into words (assign meaning to word)
â—‹Break into sentences (put word meanings back to sentence meaning)
3
Word Tokenization

Page 4:
Tokenization - Thai
â—Thai has no space between words
â—Thai has no clear sentence boundaries
4
http://www.starwars.siligon.com/swcong.html 

Page 5:
Tokenization - Thai
Social media text
#à¸ªà¸•à¸­à¸£à¸µà¹ˆà¸‚à¸­à¸‡à¹‚à¸¡ ğŸ˜ŠğŸ™ #Days23ofMobile ğŸŒˆ ...23 à¸§à¸±à¸™à¹à¸¥ïœ‹à¸§à¸™à¸°à¹€à¸ˆïœ‹à¸²à¹‚à¸¡ à¸à¸µà¹ˆà¸à¹à¸²à¸¥à¸±à¸‡à¸„à¸´à¸”à¸–à¸¶à¸‡à¸«à¸™à¸¹à¸­à¸¢à¸¹ïœŠà¹€à¸¥à¸¢ à¹à¸«à¸™à¸° à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¸«à¸²à¸¢à¹€à¸¥à¸¢
à¸™à¸° ğŸ™ˆ à¸à¸µà¹ˆà¸à¹‡à¸§ïœŠà¸²à¸«à¸²à¸¢à¹„à¸›à¹„à¸«à¸™à¹à¸­à¸šà¸«à¸™à¸µà¹„à¸›à¹€à¸¥ïœŠà¸™à¸¥à¸­à¸‡à¸šà¸­à¸£ïœà¸”à¸™à¸µà¹ˆà¹€à¸­à¸‡ à¹€à¸¥ïœŠà¸™à¸£à¸°à¸§à¸±à¸‡à¹†à¸™à¸°à¸à¸µà¹ˆà¹€à¸›ïœ’à¸™à¸«ïœŠà¸§à¸‡ à¹€à¸”à¸§à¸¥ïœ‹à¸¡à¹à¸–à¸¡à¹„à¸¡ïœŠà¸¡à¸µà¸•à¸°à¸«à¸¥à¸²à¸¡à¸„à¸­à¸¢à¹€à¸¥ïœŠà¸™
à¹€à¸›ïœ’à¸™à¹€à¸à¸·à¹ˆà¸­à¸™à¸­à¸µà¸ ğŸ˜† à¸à¸µà¹ˆà¸ˆà¸°à¸šà¸­à¸à¸§ïœŠà¸² "à¸«à¸™à¸¹à¸­à¸¢ïœŠà¸²à¸¥à¸·à¸¡à¸¥à¸‡à¸Šà¸·à¹ˆà¸­à¹€à¸¥à¸·à¸­à¸à¸•à¸±à¹‰à¸‡à¸™à¸°" à¹€à¸”à¸µà¹‹à¸¢à¸§à¸à¸§à¸à¸à¸µà¹ˆà¸‚à¹à¸²à¸à¸±à¸™à¹„à¸¡ïœŠà¸­à¸­à¸à¸™à¸° 5555 à¸à¸²à¸à¹„à¸§ïœ‹à¸à¸±à¸™à¸¥à¸·à¸¡ à¸¢à¸´à¹ˆà¸‡à¹€à¸”ïœà¸­
à¹†à¸­à¸¢à¸¹ïœŠ à¸­à¸µà¸à¸­à¸¢ïœŠà¸²à¸‡à¹†à¸«à¸™à¸¹à¸•ïœ‹à¸­à¸‡à¸à¸¥à¸±à¸šà¸¡à¸²à¸™à¸°à¸à¸§à¸à¹€à¸£à¸²à¸£à¸­à¸«à¸™à¸¹à¸­à¸¢à¸¹ïœŠ ğŸ˜Š à¸à¸µà¹ˆà¸™à¸µà¹ˆà¹€à¸•à¸£à¸µà¸¢à¸¡à¸£à¸­à¹‚à¸«à¸§à¸•à¹ƒà¸«ïœ‹à¸«à¸™à¸¹à¹€à¸•à¹‡à¸¡à¸—à¸µà¹ˆà¹€à¸¥ïœ‹à¸¢ ğŸ˜‹ à¹„à¸›à¹€à¸£à¸µà¸¢à¸™à¹€à¸›ïœ’à¸™à¹„à¸‡à¸šïœ‹à¸²à¸‡ 
à¸¢à¸±à¸‡à¹€à¸«à¸‡à¸²à¸­à¸¢à¸¹ïœŠà¸¡à¸±à¹‰à¸¢ à¹à¸•ïœŠà¸à¸µà¹ˆà¸§ïœŠà¸²à¸„à¸‡à¹„à¸¡ïœŠà¹à¸¥ïœ‹à¸§à¸«à¸¥à¸°à¸¡à¸±à¹‰à¸‡ à¸§à¸±à¸™à¸™à¸µà¹‰à¸‚à¸¶à¹‰à¸™à¸ªà¹€à¸•à¸ˆà¸­à¸µà¸à¹à¸¥ïœ‹à¸§à¸ªà¸´à¸™à¸° à¹€à¸«à¸™à¸·à¹ˆà¸­à¸¢à¸¡à¸±à¹‰à¸¢à¸„à¸° à¹à¸•ïœŠà¸„à¸‡à¸ªà¸™à¸¸à¸à¸¡à¸²à¸à¸à¸§ïœŠà¸²à¸­à¸¢à¸¹ïœŠà¹à¸¥ïœ‹à¸§à¹€à¸™à¸­à¸° ğŸ˜ 
à¹à¸„ïœŠà¹„à¸”ïœ‹à¹€à¸«à¹‡à¸™à¸£à¸­à¸¢à¸¢à¸´à¹‰à¸¡à¸‚à¸­à¸‡à¸«à¸™à¸¹à¸à¸µà¹ˆà¸à¹‡à¸ªà¸šà¸²à¸¢à¹ƒà¸ˆà¹à¸¥à¸° à¹à¸•ïœŠà¹€à¸­ïœŒà¸° à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¹ƒà¸„à¸£à¸šïœŠà¸™à¸­à¸¢à¸²à¸à¸à¸¥à¸±à¸šà¸šïœ‹à¸²à¸™à¸­à¸µà¸à¹à¸¥ïœ‹à¸§à¸™à¸° à¸­à¸”à¸”à¸¹ the toy à¹€à¸¥à¸¢à¸­ïœŠà¸°à¸”à¸´à¹‰ 
ğŸ˜† à¸«à¸§ïœŠà¸²à¸¢à¹†à¹†à¹† à¸™ïœŠà¸²à¸ªà¸‡à¸ªà¸²à¸£ 555 à¹‚à¸­à¸à¸²à¸ªà¸«à¸™ïœ‹à¸²à¸¢à¸±à¸‡à¸¡à¸µà¸™à¸° à¸•à¸±à¹‰à¸‡à¹ƒà¸ˆà¸—à¹à¸²à¸‡à¸²à¸™à¸™à¸°à¸„à¸° à¹€à¸ˆïœ‹à¸²à¸«à¸¥à¸²à¸¡à¹à¸Ÿà¸™à¸«à¸™à¸¹à¸à¹‡à¸­à¸” à¹„à¸¡ïœŠà¹„à¸”ïœ‹à¸­à¸”à¸„à¸™à¹€à¸”à¸µà¸¢à¸§
à¸‹à¸°à¸«à¸™ïœŠà¸­à¸¢à¸™à¸° 555 ğŸ™Š à¸”à¸¹à¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸ˆà¸±à¸‡à¸™ïœ‹à¸²à¹€à¸ˆïœ‹à¸²à¸•à¸±à¸§à¹€à¸¥à¹‡à¸ ğŸ˜ŠğŸ’— à¸„à¸´à¸”à¸–à¸¶à¸‡à¸™à¸° à¹€à¸”à¸µà¹‹à¸¢à¸§à¸à¹‡à¹„à¸”ïœ‹ 2shot à¸à¸±à¸™à¹à¸¥ïœ‹à¸§ à¸à¸µà¹ˆà¹‚à¸„à¸•à¸£à¸•à¸·à¹ˆà¸™à¹€à¸•ïœ‹à¸™à¹€à¸¥à¸¢ 
à¸•à¸·à¹ˆà¸™à¹€à¸•ïœ‹à¸™à¸à¸§ïœŠà¸²à¹„à¸›à¸ˆà¸±à¸šà¸¡à¸·à¸­à¹€à¸¢à¸­à¸° ğŸ™ˆ à¸„à¸´à¸”à¸—ïœŠà¸²à¹„à¸¡ïœŠà¸­à¸­à¸à¹€à¸¥ïœ‹à¸¢ à¸¡à¸µà¸—ïœŠà¸²à¹„à¸«à¸™à¹à¸™à¸°à¸™à¹à¸²à¸¡à¸±à¹‰à¸¢ à¸ŠïœŠà¸§à¸¢à¸à¸µà¹ˆà¸«à¸™ïœŠà¸­à¸¢à¸¢à¸¢à¸¢à¸¢ ğŸ˜…... #Mobilebnk48 #à¸•à¸¹ïœ‹
à¹€à¸à¸¥à¸‡à¹‚à¸¡à¸šà¸´à¸¥ #à¸Šà¸²à¸§à¹€à¸«à¸£à¸µà¸¢à¸à¸«à¸¢à¸­à¸”à¸•à¸¹ïœ‹ #MOTA09
5

Page 6:
Tokenization - Thai
â—Many word boundaries depends on the context (meaning)
â—Even amongst Thais the definition of word boundary is unclear
â—‹Needs a consensus when designing a corpus
â—‹Sometimes depends on the application
â– Linguist vs machine learning concerns
6
à¸•à¸² à¸à¸¥à¸¡ vs à¸•à¸²à¸ à¸¥à¸¡
à¸„à¸“à¸°à¸à¸£à¸£à¸¡à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸–à¸™à¸™

Page 7:
Dictionary-based vs Machine-learning-based
â—1) Dictionary-based
â—‹Longest matching
â—‹Maximal matching
â—2) Machine-learning-based
7

Page 8:
Dictionary-based word segmentation
â—
Perform by scanning a string and matching each substring against words from a dictionary.  
(No dataset needed, just prepare a dictionary!)
â—
However, there is ambiguity in matching. (There are many many ways to match.)
â—
So, matching methods are developed: 
â—‹
1. Longest matching
â—‹
2. Maximal matching
8
à¸›ïœ†à¸²à¸¢à¸à¸¥à¸±à¸šà¸£à¸–
Meknavin, Surapant, Paisarn Charoenpornsawat, and Boonserm Kijsirikul. "Feature-based Thai word segmentation." Proceedings of Natural Language Processing Pacific Rim Symposium. Vol. 97. 1997.

Page 9:
1) Longest Matching
â€¢ Scan a sentence from left to right
â€¢ Keep finding a word from the starting point until no word matches (longest), then move to the 
next point
â€¢ Backtrack if current segmentation leads to an un-segmentable chunk
â€¢ à¸›ïœ†à¸²à¸¢à¸à¸¥à¸±à¸šà¸£à¸–
Start scanning with â€œà¸›â€ as the starting point
â€¢ à¸›ïœ†à¸²à¸¢à¸à¸¥à¸±à¸šà¸£à¸–
Keep scanning â€¦
â€¢ à¸›ïœ†à¸²à¸¢/à¸à¸¥à¸±à¸šà¸£à¸–
No more words start with â€œà¸›ïœ†à¸²à¸¢â€, move to the next point
â€¢ â€¦
â€¢ à¸›ïœ†à¸²à¸¢/à¸à¸¥à¸±à¸š/à¸£à¸–
9

Page 10:
2) Maximal Matching
â€¢ Generate all possible segmentations
â€¢ Select the segmentations with the fewest words
10
Haruechaiyasak, Choochart, Sarawoot Kongyoung, and Matthew Dailey. "A comparative study on thai word segmentation approaches." Electrical Engineering/Electronics, Computer, 
Telecommunications and Information Technology, 2008. ECTI-CON 2008. 5th International Conference on. Vol. 1. IEEE, 2008.

Page 11:
What if?
â€¢ What if there are more than one segmentation with the fewest words?
â€¢ Other heuristics are applied, for example
â€¢ Language model score 
11

Page 12:
Maximal matching
â€¢ Maximal matching can be done using dynamic programming.
â€¢ Let  d(i,j)  be the function that returns the number of the fewest words possible, with the last word 
starting with ith character (row) and ending with jth character (column). It can be defined as:
when c[i..j] is a string of words in the sentence (assume it is started at index 1) and the base case is 
d(1,1) = 1.
12

Page 13:
Maximal matching: Initialization (1st row)
â€¢ Maximal matching can be done using dynamic programming.
â€¢ Let  d(i,j)  be the function that returns the number of the fewest words possible, with the last word 
starting with ith character (row) and ending with jth character (column). It can be defined as:
when c[i..j] is a string of words in the sentence (assume it is started at index 1) and the base case is 
d(1,1) = 1.
13
1st row and it is a word

Page 14:
Maximal matching: Find a word in dictionary
â€¢ Maximal matching can be done using dynamic programming.
â€¢ Let  d(i,j)  be the function that returns the number of the fewest words possible, with the last word 
starting with ith character (row) and ending with jth character (column). It can be defined as:
when c[i..j] is a string of words in the sentence (assume it is started at index 1) and the base case is 
d(1,1) = 1.
14
If last word is a word

Page 15:
Maximal matching: Check all possible 
segmentations before the ï¬nal word
â€¢ Maximal matching can be done using dynamic programming.
â€¢ Let  d(i,j)  be the function that returns the number of the fewest words possible, with the last word 
starting with ith character (row) and ending with jth character (column). It can be defined as:
when c[i..j] is a string of words in the sentence (assume it is started at index 1) and the base case is 
d(1,1) = 1.
15
Check all possible segmentations before the final word
Check the whole column in the previous row

Page 16:
Maximal matching: The ï¬nal word
â€¢ Maximal matching can be done using dynamic programming.
â€¢ Let  d(i,j)  be the function that returns the number of the fewest words possible, with the last word 
starting with ith character (row) and ending with jth character (column). It can be defined as:
when c[i..j] is a string of words in the sentence (assume it is started at index 1) and the base case is 
d(1,1) = 1.
16
The final word

Page 17:
Example (1)
17
5
4
âˆ

Page 18:
Example (2) 
18
5
4
âˆ

Page 19:
Example (3)
19
5
4
âˆ

Page 20:
Example (4)
20
5
4
âˆ

Page 21:
Example (5):
Backtracking
21
5
4
âˆ

Page 22:
22
https://pythainlp.org/docs/5.0/api/tokenize.html 

Page 23:
How to improve the tokenizer? NN
 Predict 1/0 for each character
23
Character Tokens
String-to-Integer (stoi) + Pad
Predict Beginning of Word
Complicated Function (NN)
Use neural nets to tokenize?
Reference: http://web.archive.org/web/20181005101442/https://sertiscorp.com/thai-word-segmentation-with-bi-directional_rnn/ 

Page 24:
NN-based Tokenizers Performance
24
https://github.com/PyThaiNLP/attacut 

